\chapter{Conclusion}

In this thesis we presented a massively parallel approach for computing the GIT fan of a torus action on an affine variety. After explaining the mathematical background and the algorithmic idea of \citeauthor{gitfan_symmetry}, our realisation of the algorithm has been discussed in detail. \gpispace{} is the key component when it comes to developing an application that executes the GIT fan algorithm on large scale systems such as the cluster system of the \ac{Fraunhofer ITWM}. Concurrent parts have been reformulated in terms of Petri nets, whereas the sequential core operations are carried out by the computer algebra system \singular{}.

We have shown that the computation of the GIT fan translates to a graph traversal problem which is easily parallelised by expanding frontier nodes simultaneously. However, it is crucial that every computing node has a global view of the frontier set. Otherwise, nodes may be expanded several times, wasting potential computation time on the cluster system. For this reason we provided several storage implementations that manage a list of expanded nodes and frontier nodes. In our use case -- the computation of the Mori chamber decomposition of $\Mov(\overline{M}_{0,6})$ -- an in\=/memory solution on a single node, which is accessible via a RPC-server,  prevailed.

Our implementation scales almost linearly with the amount of available cores, obtaining a utilisation of still 80\% when using 640 cores. We were able to determine the Mori chamber representatives in approximately 21 minutes from a precomputed set of representatives for the faces of the simplex $\gamma$ under the $\mathcal{S}_6$\=/action. Previous implementations executed on a single machine with 16 cores terminated after a whole day. \cite[Rermark 6.8]{gitfan_symmetry}

In order to determine GIT fans with feasible effort, \citeauthor{gitfan_symmetry} introduced a fast monomial containment test and exploited symmetries occurring in the problem formulation. We contributed to this goal by optimising existing code and rewriting it for scalable hardware systems. However, some optimisations still are left open and may be revisited in future work. For one thing, we still rely on a precomputation by \gap{} that determines all representatives of the faces of $\gamma$. Although we provide a routine for this task, it does not utilise several cores and is too time\=/consuming when applied to the \msix{} example. Parallel implementations would be of high interest, increasing the scalability of our implementation further. Furthermore, our storage solution fails if the memory requirements -- given by the size of the GIT fan -- exceed the available RAM of a single node. The development of a distributed hash table with load balancing features would make the RAM of all nodes available such that scaling memory requirements may be satisfied without falling back to significant slower hard drives. This becomes particularly interesting in scenarios where cones cannot be encoded by short bit vectors of fixed length. Storing generating rays or defining half spaces requires an uncontrollable amount of memory due to exact representations of numbers in computer algebra systems.