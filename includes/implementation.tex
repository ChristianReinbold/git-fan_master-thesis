\chapter{Implementation}

In the following we present the implementation of the algorithm discussed in the previous chapter. In contrast to the existing \singular{} implementation \emph{gitfan.lib} \cite{gitfan}, which forms the foundation of our work, the approach described here utilises high performance computing methods in order to speed up the execution by running the algorithm on any number of machines simultaneously. Naturally, one has to identify independent components permitting concurrent execution on the one hand and inherent sequential processes on the other hand. As the sequential parts of the algorithm mostly agree with \emph{gitfan.lib} -- only signatures have been modified, providing an appropriate interface for the superordinate parallel environment -- we skip a detailed discussion of those and refer the interested reader to \cite{gitfan}. The chapter is structured as follows: First, we discuss the employed parallelisation framework \gpispace{} developed by the \ac{Fraunhofer ITWM} and follow up with the integration of singular code into \gpispace{} applications. Next, a parallel design of algorithm \todo{(ref needed to main algorithm in previous chapter)} is presented. A separate section covers approaches for managing found GIT cones, a potential bottleneck due to data dependencies. Finally, we describe input and output formats, additional features such as speeding up the execution by precomputed results and executed software tests in order to verify the functional correctness of the program.

\section{\gpispace}

\gpispace{} is a workflow management system developed by the \ac{Fraunhofer ITWM} which supports the execution of arbitrary workflows on ultra scale systems \cite{gpispace}. It consists of three key components: 
\begin{itemize}
	\item the \ac{DRTS}, which is responsible for building and managing arbitrary worker topologies based on the available compute nodes, resource management and job scheduling. It supports the reallocation of jobs in case of hardware failures and further integrates dynamically added hardware
	\item the \ac{WE}. It tracks the state of the workflow, identifies a front of activities for which all dependencies are resolved and laces jobs from input data and active transitions which then are sent to the \ac{DRTS} for scheduling.
	\item a virtual memory layer, which provides the necessary infrastructure for the \ac{PGAS} programming model. It relies on \textsc{GPI} by \ac{Fraunhofer ITWM} \cite{gpi}.
\end{itemize}

The framework has been developed with separation of concerns in mind, whereby the concerns here are given by computation and coordination \cite{coordination_language}. In our case the computation takes place in sequential \singular-routines. The dependencies and data transfers between this routines, which assemble the particular routines into a complex, parallel algorithm, are described by a special domain language in the the coordination layer. In \gpispace{}, this domain language is chosen to be an extension of the well known Petri net model introduced in 1962 by Carl Adam Petri \cite{petri}. Its formal nature permits a vast range of analysis and verification techniques. Concurrency is easily described due to locality of states. Furthermore, Petri nets yield precise execution semantics, supporting interruptions and restarts by differentiating between activation and execution of functional units. For this reason, fault tolerance to hardware failures is achieved by simply restarting failed executions. \cite{petri_net_wms}

\gpispace{} is shipped with an appropriate compiler that generates all files which are necessary for execution from a XML description of a given Petri net. Furthermore, \gpispace{} also provides a basic visualization tool that comes in handy when debugging small to medium sized nets.

\subsection*{Petri nets}

Formally, an (unweighted) \emph{Petri net} is a triple $(P, T, F)$ of \emph{places} $P$, \emph{transitions} $T$ and directed \emph{arcs} $F\subseteq (P\times T)\cup(T\times P)$ relating the former concepts. We demand that $P\cap T = \emptyset$. A function $M\colon P\rightarrow\natural$ is called \emph{marking} and describes a possible state of the Petri net. If we have $M(p)=k$ for a place $p$ and the current state of the Petri net is given by $M$, we say that $p$ holds $k$ \emph{tokens}. Visually, circles depict places, rectangles are transitions and arrows between circles and rectangles represent arcs whose direction is indicated by the tip. The current state $M$ of the Petri net is displayed by placing $M(p)$ dots in the circle representing the place $p$. Figure~\ref{net:sample} gives an example for the graphical representation of a Petri net.

\input{graphics/petri_net_sample}

A transition $t$ is said to be \emph{active}, iff
$$\forall p\in P\colon (p,t)\in F\Rightarrow M(p) > 0$$
holds. In this case \emph{firing} $t$ means to transform the current state $M$ into $M'$ by the update rule
$$M'(p) \defeq \begin{cases}
M(p) - 1 & (p,t)\in F,\ (t,p)\notin F \\
M(p) + 1 & (p,t)\notin F,\ (t,p)\in F \\
M(p) & \text{else}
\end{cases}$$
for all $p\in P$. We interpret this as follows: For every incoming arc $(p,t)$ from $p$ a token in $p$ is consumed and for every outgoing arc $(t, p')$ to $p'$ a new token is placed into $p'$. Note that the notion of tokens ``moving through transitions'' instead of consumption and creation of tokens is erroneous as we will see later on when attaching data to them.

\input{graphics/petri_net_sample_concurrency}

An important property of Petri nets is given by the fact that in many cases the same result is obtained if the order in which two transitions $t_1$ and $t_2$ fire is reversed. This allows the modelling of concurrent behaviour. When executing the Petri net model on a machine, the necessary steps for firing $t_1$ and $t_2$ respectively may be performed in parallel. Figure~\ref{net:sample_concurrency} depicts such a situation. Note that figure~\ref{net:sample} also shows an example for concurrent behaviour in which the transition may fire twice. However, the order in which the tokens in the left place are consumed is of no relevance. This example seems trivial as tokens located at the same place are indistinguishable. However, this is not the case anymore when attaching data to tokens. In fact, figure~\ref{net:sample} depicts the common scenario of concurrent behaviour in our algorithm, as parallel execution is mostly achieved by invoking the same routine for varying, independent input data.

\subsection*{Coloured Petri nets and guards}

Workflows describe a (possible concurrent) progression of processes, hence it is evident to model a process by a transition. The input and output data of a process is taken into account by attaching data values of a predefined type to each token. Then every incoming arc provides input data for the process corresponding to the target transition, whereas every outgoing arc describes output data of the source transition that should be put into the target place. A place may only hold tokens of a single type, introducing strong typing to Petri nets. This kind of Petri net is called \emph{coloured Petri net}, developed by Kurt Jensen in 1980. The term originates from the analogy of understanding each distinct type as a unique color. A formal treatment of this concept may be found in \cite{cp_nets}.

In order to control the data flow, each transition may be tagged with a predicate over the types of places with incoming arcs, which is called \emph{condition} or \emph{guard} \cite{cp_nets}. The activation rule for a transition is extended such that not only a token has to be present for every incoming arc, but also the data values of the consumed tokens have to satisfy the condition. \gpispace{} provides its own expression language in order to specify conditions. Figure~\ref{net:sample_guard} shows a transition that consumes tokens with positive data values only.

\input{graphics/petri_net_sample_guard}

\subsection*{Expressions}

\gpispace{} supports two kinds of transitions. The first kind triggers a module call whenever firing, executing arbitrary \cplusplus-code where the input and output tokens are mapped to references. The module call is packaged as a job and scheduled by the \ac{DRTS}. A lightweight approach that requires no scheduling is given by the second kind of transition. The expression language provided by \gpispace{} allows to specify the output data values of a transition by means of simple operations on the input data values. This extension is intended to be used for minor computations and saves the overhead of generating and scheduling a job by executing the expression directly within the \ac{WE}. For instance, counting consumed tokens may be realised that way, see figure~\ref{net:sample_expression}. If the counting would be realised with \cplusplus, each increment will cause a disproportionate communication and scheduling overhead.

\input{graphics/petri_net_sample_expression}

\subsection*{Executing Petri nets}

The \ac{WE} is responsible for executing a given Petri net. It manages its current state and identifies active transitions. Whenever such a transition is detected, its input tokens are consumed, bundling the attached data values into a job that describes the execution of the transition. Then, this job is submitted to the \ac{DRTS} where it awaits its execution. The set of active transitions not being executed already is called front of activities and generally contains numerous elements at once due to the locality of Petri nets. Thus, the \ac{DRTS} is able to utilise the available capacities by scheduling multiple jobs in parallel.

When a job finishes, the results are returned to the \ac{WE} which extracts the output tokens and updates the state of the Petri net accordingly. Note that during the whole process described above, the input tokens for an active transition are consumed whereas the output tokens are not available yet. In order to fully describe the state of an executable Petri net, one has to incorporate the knowledge about all active transitions and its cached tokens into the state description. Hence, the \ac{WE} has to manage \emph{timed}, coloured Petri nets. Figure~\ref{net:sample_execution} illustrates the process of executing two active transitions in parallel. The module called by the transition sleeps for $x$ seconds, where $x$ is the input data value, and then returns the increment $x + 1$.

\input{graphics/petri_net_sample_execution}

The \ac{WE} itself is executed on a single node of the underlying system. Hence, special care must be taken when designing Petri nets. Instead of firing numerous transitions with execution times of milliseconds, one is advised to aggregate these transitions into a single one with an execution time of several seconds. Otherwise, the \ac{WE} quickly becomes a bottleneck since the amount of scheduled transitions per second does not scale with the number of available computing nodes. Furthermore, the marking of the Petri net is stored in the RAM of a single node. If large quantities of data have to be managed, the tokens should only contain a reasonable amount of metadata pointing to the real data which -- for instance -- is stored in a shared directory.

\gpispace{} assumes that module calls have no side effects. Thus, when implementing the execution code for the transitions, possible racing conditions and concurrency issues have to be considered properly. Furthermore, each worker managed by the \ac{DRTS} is executed in its own process, but beyond that, jobs assigned to the same worker run in the same environment. For this reason jobs are influenced by changes of global state by prior executions. This behaviour causes several issues when integrating \singular{}, which is implemented as a large, global state machine.

\section{Integration of Singular}
drawbacks

\section{Application flow \& Concurrency}

\section{GIT cone data structure}

\section{Input \& Output structure}

\section{Additional features}
an Kommandozeilenparameter entlanghangeln

\section{Software testing}