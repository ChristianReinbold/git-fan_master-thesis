\chapter{Implementation}

In the following we present the implementation of the algorithm discussed in the previous chapter. In contrast to the existing \singular{} implementation \emph{gitfan.lib} \cite{gitfan}, which forms the foundation of our work, the approach described here utilises high performance computing methods in order to speed up the execution by running the algorithm on any number of machines simultaneously. Naturally, one has to identify independent components permitting concurrent execution on the one hand and inherent sequential processes on the other hand. As the sequential parts of the algorithm mostly agree with \emph{gitfan.lib} -- only signatures have been modified, providing an appropriate interface for the superordinate parallel environment -- we skip a detailed discussion of those and refer the interested reader to \cite{gitfan}. The chapter is structured as follows: First, we discuss the employed parallelisation framework \gpispace{} developed by the \ac{Fraunhofer ITWM} and follow up with the integration of singular code into \gpispace{} applications. Next, a parallel design of algorithm~\ref{algorithm:main} is presented. A separate section covers approaches for managing found GIT cones, a potential bottleneck due to data dependencies. Finally, we describe input and output formats, additional features such as speeding up the execution by precomputed results and executed software tests in order to verify the functional correctness of the program.

\section{\gpispace}

\gpispace{} is a workflow management system developed by the \ac{Fraunhofer ITWM} which supports the execution of arbitrary workflows on ultra scale systems \cite{gpispace}. It consists of three key components: 
\begin{itemize}
	\item the \ac{DRTS}, which is responsible for building and managing arbitrary worker topologies based on the available compute nodes, resource management and job scheduling. It supports the reallocation of jobs in case of hardware failures and further integrates dynamically added hardware
	\item the \ac{WE}. It tracks the state of the workflow, identifies a front of activities for which all dependencies are resolved and laces jobs from input data and active transitions which then are sent to the \ac{DRTS} for scheduling.
	\item a virtual memory layer, which provides the necessary infrastructure for the \ac{PGAS} programming model. It relies on \textsc{GPI} by \ac{Fraunhofer ITWM} \cite{gpi}.
\end{itemize}

The framework has been developed with separation of concerns in mind, whereby the concerns here are given by computation and coordination \cite{coordination_language}. In our case the computation takes place in sequential \singular-routines. The dependencies and data transfers between this routines, which assemble the particular routines into a complex, parallel algorithm, are described by a special domain language in the the coordination layer. In \gpispace{}, this domain language is chosen to be an extension of the well known Petri net model introduced in 1962 by Carl Adam Petri \cite{petri}. Its formal nature permits a vast range of analysis and verification techniques. Concurrency is easily described due to locality of states. Furthermore, Petri nets yield precise execution semantics, supporting interruptions and restarts by differentiating between activation and execution of functional units. For this reason, fault tolerance to hardware failures is achieved by simply restarting failed executions. \cite{petri_net_wms}

\gpispace{} is shipped with an appropriate compiler that generates all files which are necessary for execution from a XML description of a given Petri net. Furthermore, \gpispace{} also provides a basic visualization tool that comes in handy when debugging small to medium sized nets.

\subsection*{Petri nets}

Formally, an (unweighted) \emph{Petri net} is a triple $(P, T, F)$ of \emph{places} $P$, \emph{transitions} $T$ and directed \emph{arcs} $F\subseteq (P\times T)\cup(T\times P)$ relating the former concepts. We demand that $P\cap T = \emptyset$. A function $M\colon P\rightarrow\natural$ is called \emph{marking} and describes a possible state of the Petri net. If we have $M(p)=k$ for a place $p$ and the current state of the Petri net is given by $M$, we say that $p$ holds $k$ \emph{tokens}. Visually, circles depict places, rectangles are transitions and arrows between circles and rectangles represent arcs whose direction is indicated by the tip. The current state $M$ of the Petri net is displayed by placing $M(p)$ dots in the circle representing the place $p$. Figure~\ref{net:sample} gives an example for the graphical representation of a Petri net.

\input{graphics/petri_net_sample}

A transition $t$ is said to be \emph{active}, iff
$$\forall p\in P\colon (p,t)\in F\Rightarrow M(p) > 0$$
holds. In this case \emph{firing} $t$ means to transform the current state $M$ into $M'$ by the update rule
$$M'(p) \defeq \begin{cases}
M(p) - 1 & (p,t)\in F,\ (t,p)\notin F \\
M(p) + 1 & (p,t)\notin F,\ (t,p)\in F \\
M(p) & \text{else}
\end{cases}$$
for all $p\in P$. We interpret this as follows: For every incoming arc $(p,t)$ from $p$ a token in $p$ is consumed and for every outgoing arc $(t, p')$ to $p'$ a new token is placed into $p'$. Note that the notion of tokens ``moving through transitions'' instead of consumption and creation of tokens is erroneous as we will see later on when attaching data to them.

\input{graphics/petri_net_sample_concurrency}

An important property of Petri nets is given by the fact that in many cases the same result is obtained if the order in which two transitions $t_1$ and $t_2$ fire is reversed. This allows the modelling of concurrent behaviour. When executing the Petri net model on a machine, the necessary steps for firing $t_1$ and $t_2$ respectively may be performed in parallel. Figure~\ref{net:sample_concurrency} depicts such a situation. Note that figure~\ref{net:sample} also shows an example for concurrent behaviour in which the transition may fire twice. However, the order in which the tokens in the left place are consumed is of no relevance. This example seems trivial as tokens located at the same place are indistinguishable. However, this is not the case anymore when attaching data to tokens. In fact, figure~\ref{net:sample} depicts the common scenario of concurrent behaviour in our algorithm, as parallel execution is mostly achieved by invoking the same routine for varying, independent input data.

\subsection*{Coloured Petri nets and guards}

Workflows describe a (possible concurrent) progression of processes, hence it is evident to model a process by a transition. The input and output data of a process is taken into account by attaching data values of a predefined type to each token. Then every incoming arc provides input data for the process corresponding to the target transition, whereas every outgoing arc describes output data of the source transition that should be put into the target place. A place may only hold tokens of a single type, introducing strong typing to Petri nets. This kind of Petri net is called \emph{coloured Petri net}, developed by Kurt Jensen in 1980. The term originates from the analogy of understanding each distinct type as a unique color. A formal treatment of this concept may be found in \cite{cp_nets}.

In order to control the data flow, each transition may be tagged with a predicate over the types of places with incoming arcs, which is called \emph{condition} or \emph{guard} \cite{cp_nets}. The activation rule for a transition is extended such that not only a token has to be present for every incoming arc, but also the data values of the consumed tokens have to satisfy the condition. \gpispace{} provides its own expression language in order to specify conditions. Figure~\ref{net:sample_guard} shows a transition that consumes tokens with positive data values only.

\input{graphics/petri_net_sample_guard}

\subsection*{Expressions}

\gpispace{} supports two kinds of transitions. The first kind triggers a module call whenever firing, executing arbitrary \cplusplus code where the input and output tokens are mapped to references. The module call is packaged as a job and scheduled by the \ac{DRTS}. A lightweight approach that requires no scheduling is given by the second kind of transition. The expression language provided by \gpispace{} allows to specify the output data values of a transition by means of simple operations on the input data values. This extension is intended to be used for minor computations and saves the overhead of generating and scheduling a job by executing the expression directly within the \ac{WE}. For instance, counting consumed tokens may be realised that way, see figure~\ref{net:sample_expression}. If the counting would be realised with \cplusplus, each increment will cause a disproportionate communication and scheduling overhead.

\input{graphics/petri_net_sample_expression}

\subsection*{Executing Petri nets}

The \ac{WE} is responsible for executing a given Petri net. It manages its current state and identifies active transitions. Whenever such a transition is detected, its input tokens are consumed, bundling the attached data values into a job that describes the execution of the transition. Then, this job is submitted to the \ac{DRTS} where it awaits its execution. The set of active transitions not being executed already is called front of activities and generally contains numerous elements at once due to the locality of Petri nets. Thus, the \ac{DRTS} is able to utilise the available capacities by scheduling multiple jobs in parallel.

When a job finishes, the results are returned to the \ac{WE} which extracts the output tokens and updates the state of the Petri net accordingly. Note that during the whole process described above, the input tokens for an active transition are consumed whereas the output tokens are not available yet. In order to fully describe the state of an executable Petri net, one has to incorporate the knowledge about all active transitions and its cached tokens into the state description. Hence, the \ac{WE} has to manage \emph{timed}, coloured Petri nets. Figure~\ref{net:sample_execution} illustrates the process of executing two active transitions in parallel. The module called by the transition sleeps for $x$ seconds, where $x$ is the input data value, and then returns the increment $x + 1$.

\input{graphics/petri_net_sample_execution}

The \ac{WE} itself is executed on a single node of the underlying system. Hence, special care must be taken when designing Petri nets. Instead of firing numerous transitions with execution times of milliseconds, one is advised to aggregate these transitions into a single one with an execution time of several seconds. Otherwise, the \ac{WE} quickly becomes a bottleneck since the amount of scheduled transitions per second does not scale with the number of available computing nodes. Furthermore, the marking of the Petri net is stored in the RAM of a single node. If large quantities of data have to be managed, the tokens should only contain a reasonable amount of metadata pointing to the real data which -- for instance -- is stored in a shared directory.

\gpispace{} assumes that module calls have no side effects. Thus, when implementing the execution code for the transitions, possible racing conditions and concurrency issues have to be considered properly. Furthermore, each worker managed by the \ac{DRTS} is executed in its own process, but beyond that, jobs assigned to the same worker run in the same environment. For this reason jobs are influenced by changes of global state by prior executions. This behaviour causes several issues when integrating \singular{}, which is implemented as a large, global state machine.

\section{Integration of Singular}
\singular{} is a computer algebra system for polynomial computations, with special emphasis on commutative and non\hbox{-}commutative, algebraic geometry and singularity theory \cite{singular}. Typically, algorithms utilising \singular{} are written in its own \c\hbox{-}like programming language, which is interpreted at runtime. Besides providing a console frontend for code execution, \singular{} also comes with the \c{} library \libsingular{} that implements its full backend. In particular, \c{} methods for interpreting and executing \singular{} code are available. \libsingular{} also exposes the internal data structures used by \singular{}, so that one can write conversion methods between \singular{} types and user defined types, i.e. \gpispace{} types.

Since module calls in \gpispace{} permit the execution of arbitrary \cplusplus{} code, every legacy application with a \c{} interface such as \singular{} can easily be integrated and executed. In order to execute \singular{} code in a module call, one has to perform the following steps:
\begin{enumerate}
	\item Check if a \singular{} instance is already running on the current worker. If not, initialise \singular{}.
	\item Convert \gpispace{} types to \singular{} types.
	\item Map the converted data to identifiers in order to address it by \singular{} code.
	\item Formulate a \c{} string containing the \singular{} code to be executed and pass it to the \singular{} interpreter via \libsingular{}. Note that instead of hard coding large blocks of code, it is reasonable to outsource them into a \singular{} library. Then, this step reduces to loading the library and invoking a single method.
	\item Extract the data mapped to all identifiers that contain results of the computation.
	\item Convert the extracted \singular{} types to \gpispace{} types that can be attached to tokens.
\end{enumerate}

\subsection*{Drawback: Global state}
Although integrating \singular{} code is sufficiently easy, the approach described above suffers from a serious issue that originates from \singular{} heavily relying on global state. For instance, loaded libraries, declared variables and the current basering are managed in global data structures. If module calls are supposed to have no side effects, all changes concerning global state have to be reverted before returning from them. Unfortunately, \libsingular{} lacks the feature of creating and restoring images of its global state. In fact, there seems to be no way of resetting \singular{} to its initial state short of killing the current process, which -- obviously -- is not desirable.

Not being able to reset the \singular{} instance also makes writing decoupled test cases more problematic. In order to avoid tests being influenced by one another due to global \singular{} state, each test has to be executed in a separate process. Unfortunately, this prevents the utilisation of many \gtest{} features such as bundling test cases, reusing data configurations and parametrising tests.

\subsection*{Drawback: No in memory serialization of \singular{} types}
When \gpispace{} has to share \singular{} data between module calls without knowing the internal structure of the data, this can be achieved by serialising the data into \acp{BLOB} which then are managed by the \ac{WE}. \singular{} supports the serialization of its common data types by using \emph{ssi file links}. As the name suggests, the serialization string always is written to a file. It would be desirable to also obtain a serialization string in memory without the overhead of using virtual files or reading the created ssi files. Currently, we pass metadata (that is file paths) pointing to ssi files in a shared directory instead of acquiring and passing \acp{BLOB}.

\section{Application flow \& Concurrency}

We differentiate between three phases when implementing the algorithm described in chapter~\ref{chap:algorithm}. During the first phase, preprocessing of the input data and the identification of all orbits of \afaces with full dimensional image takes place. The second phase covers the computation of the orbit cones and the group action of symmetries on GIT cones. This data is mandatory when determining the GIT fan in the third phase by traversing all maximal cones of the fan. Figure~\ref{net:algorithm_overview} depicts the simplified data flow of the algorithm in which each phase is collapsed into a single transition. Dashed arcs indicate read-only connections, that is data values of tokens in the source place are passed to the target transition without consuming them when it fires. Hence, \emph{read-only} connections give access to static data computed once (such as processed input data), without discarding it due to consumption.

\input{graphics/petri_net_algorithm_overview}

\subsection*{Phase 1: Preprocessing, \afaces}

%the input data is preprocessed by extracting the dimension parameters of the problem, computing the moving cone if necessary and 

\subsection*{Phase 2: Orbit cones, group action on GIT cones}

\subsection*{Phase 3: GIT fan traversal}




\section{GIT cone data structure}

\section{Input \& Output structure}

\section{Additional features}
an Kommandozeilenparameter entlanghangeln

\section{Software testing}